{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib as mpl\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy\n",
    "\n",
    "import geopandas\n",
    "import cartopy.io.shapereader as shpreader\n",
    "import shapely.vectorized\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME</th>\n",
       "      <th>LAB</th>\n",
       "      <th>USAGE</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alaska/N.W. Canada [ALA:1]</td>\n",
       "      <td>ALA</td>\n",
       "      <td>land</td>\n",
       "      <td>POLYGON ((-105.00000 60.00000, -168.00000 60.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amazon [AMZ:7]</td>\n",
       "      <td>AMZ</td>\n",
       "      <td>land</td>\n",
       "      <td>POLYGON ((-66.40000 -20.00000, -79.70000 -1.20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Central America/Mexico [CAM:6]</td>\n",
       "      <td>CAM</td>\n",
       "      <td>land</td>\n",
       "      <td>POLYGON ((-68.80000 11.40000, -79.70000 -1.200...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>small islands regions Caribbean</td>\n",
       "      <td>CAR*</td>\n",
       "      <td>all</td>\n",
       "      <td>POLYGON ((-68.80000 11.40000, -85.80000 25.000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Central Asia [CAS:20]</td>\n",
       "      <td>CAS</td>\n",
       "      <td>land</td>\n",
       "      <td>POLYGON ((60.00000 30.00000, 60.00000 50.00000...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              NAME   LAB USAGE  \\\n",
       "0       Alaska/N.W. Canada [ALA:1]   ALA  land   \n",
       "1                   Amazon [AMZ:7]   AMZ  land   \n",
       "2   Central America/Mexico [CAM:6]   CAM  land   \n",
       "3  small islands regions Caribbean  CAR*   all   \n",
       "4            Central Asia [CAS:20]   CAS  land   \n",
       "\n",
       "                                            geometry  \n",
       "0  POLYGON ((-105.00000 60.00000, -168.00000 60.0...  \n",
       "1  POLYGON ((-66.40000 -20.00000, -79.70000 -1.20...  \n",
       "2  POLYGON ((-68.80000 11.40000, -79.70000 -1.200...  \n",
       "3  POLYGON ((-68.80000 11.40000, -85.80000 25.000...  \n",
       "4  POLYGON ((60.00000 30.00000, 60.00000 50.00000...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipcc_regions = geopandas.read_file(\"../data/referenceRegions.dbf\")\n",
    "ipcc_regions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ndf_id</th>\n",
       "      <th>doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7574.0</td>\n",
       "      <td>486888.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7574.0</td>\n",
       "      <td>3323227.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7574.0</td>\n",
       "      <td>1358994.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7574.0</td>\n",
       "      <td>2356292.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7574.0</td>\n",
       "      <td>1552852.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ndf_id     doc_id\n",
       "0  7574.0   486888.0\n",
       "1  7574.0  3323227.0\n",
       "2  7574.0  1358994.0\n",
       "3  7574.0  2356292.0\n",
       "4  7574.0  1552852.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "place_gridcells = pd.read_csv(\"../data/study_gridcell_all_2.5.csv\")\n",
    "place_gridcells.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = 2.5\n",
    "tdf = pd.read_csv(f'../data/study_da_6 - Temperature - upper_pred_{degrees}.csv')\n",
    "tdf[\"da_var\"] = \"Temperature\"\n",
    "pdf = pd.read_csv(f'../data/study_da_6 - Precipitation - upper_pred_{degrees}.csv')\n",
    "pdf[\"da_var\"] = \"Precipitation\"\n",
    "\n",
    "dadf = pd.concat([tdf,pdf])\n",
    "\n",
    "dadf = dadf[pd.notna(dadf[\"gridcells\"]) & dadf[\"gridcells\"]>0]\n",
    "\n",
    "dadf[\"da_trend_p\"] = dadf[\"da_trend_cells\"] / dadf[\"gridcells\"]\n",
    "\n",
    "dadf[\"da_trend_cat\"] = None\n",
    "\n",
    "dadf.loc[dadf['da_trend_p']==0,\"da_trend_cat\"] = \"0==DA\"\n",
    "dadf.loc[dadf['da_data_cells']==0,\"da_trend_cat\"] = \"NA\"\n",
    "dadf.loc[dadf['da_trend_p']>0,\"da_trend_cat\"] = \"0<DA<0.5\"\n",
    "dadf.loc[dadf['da_trend_p']>0.5,\"da_trend_cat\"] = \"DA>0.5\"\n",
    "\n",
    "places =  pd.read_csv('../data/place_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>da</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13201</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1544528</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1287688</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>245321</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1549132</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id   da\n",
       "0    13201  0.0\n",
       "1  1544528  1.0\n",
       "2  1287688  0.0\n",
       "3   245321  0.0\n",
       "4  1549132  0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdf['temperature_da'] = tdf['da_trend_cells']\n",
    "pdf['precip_da'] = tdf['da_trend_cells']\n",
    "\n",
    "dadf = tdf[['id','temperature_da']].merge(pdf[['id','precip_da']], how=\"outer\").fillna(0)\n",
    "\n",
    "dadf['da'] = np.max(dadf[['temperature_da','precip_da']],axis=1)\n",
    "dadf = dadf[['id','da']]\n",
    "\n",
    "dadf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   1.,  10.,   2.,  67.,   3.,  12.,  13., 168.,  81.,   8.,\n",
       "        33., 524.,   4., 570.,  17.,   5.,  25.,  15.,   7.,   9.,   6.,\n",
       "        45., 206., 268.,  11.,  27.,  19.,  14.,  80., 392.,  18.,  34.,\n",
       "        24.,  61.,  54.,  30., 108.,  47., 213.,  52.,  28.,  48.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dadf.da.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'da'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dadf.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12291.100000000002\n",
      "12464.100000000002\n",
      "10176.9\n",
      "10457.9\n",
      "4585.8\n",
      "4744.8\n",
      "11683.5\n",
      "11976.5\n",
      "29838.4\n",
      "30281.4\n",
      "['12 - Coastal and marine Ecosystems - mean_prediction', '12 - Human and managed - mean_prediction', '12 - Mountains, snow and ice - mean_prediction', '12 - Rivers, lakes, and soil moisture - mean_prediction', '12 - Terrestrial ES - mean_prediction']\n"
     ]
    }
   ],
   "source": [
    "cat_df = pd.read_csv('../data/1_predicted_category_documents.csv')\n",
    "predictions = pd.read_csv('../data/1_document_relevance.csv')\n",
    "df = dadf.merge(cat_df.merge(predictions), how=\"outer\")\n",
    "\n",
    "pred_cats = [c for c  in df.columns if \"12 - \" in c and \" - mean_prediction\" in c]\n",
    "for c in pred_cats:\n",
    "    print(df[c].sum())\n",
    "    label = c.replace(\" - mean_prediction\",'')\n",
    "    cs = [c, c.replace('mean_prediction','lower_pred'), c.replace('mean_prediction','upper_pred')]\n",
    "    df.loc[df[label]==1,cs] = 1\n",
    "    print(df[c].sum())\n",
    "print(pred_cats)\n",
    "\n",
    "pred_cats = [\n",
    "    \"12 - Terrestrial ES - mean_prediction\",\n",
    "    \"12 - Coastal and marine Ecosystems - mean_prediction\",\n",
    "    \"12 - Mountains, snow and ice - mean_prediction\",\n",
    "    \"12 - Rivers, lakes, and soil moisture - mean_prediction\",\n",
    "    \"12 - Human and managed - mean_prediction\",\n",
    "    \"12 - Total\"\n",
    "]\n",
    "\n",
    "pcols = [\n",
    "    '0 - relevance - mean_prediction',\n",
    "    '0 - relevance - lower_pred',\n",
    "    '0 - relevance - upper_pred'\n",
    "]\n",
    "\n",
    "df.loc[df['relevant']==1,pcols]=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf = pd.read_csv(\"../data/gridcell_studies_all_2.5.csv\")\n",
    "\n",
    "ndf.loc[ndf['LON']>180,\"LON\"]-=360\n",
    "ndf['ipccreg'] = 0\n",
    "\n",
    "#df = pd.DataFrame(columns=['IPCC region', 'Documents'])\n",
    "index = pd.Index(ipcc_regions.NAME, name=\"IPCC Region\")\n",
    "\n",
    "table = pd.DataFrame(columns=['Documents'], index=index)\n",
    "\n",
    "for i, row in ipcc_regions.iterrows():\n",
    "    inplace = shapely.vectorized.contains(row.geometry,ndf['LON'],ndf['LAT'])\n",
    "    idx = np.argwhere(inplace==True)\n",
    "    \n",
    "    ndf.loc[idx[:,0],\"ipccreg\"] = i+1\n",
    "    dids = place_gridcells[place_gridcells['ndf_id'].isin(idx)]['doc_id'].unique()\n",
    "    \n",
    "    mid = df[(df['id'].isin(dids)) & (df[\"0 - relevance - mean_prediction\"]>=0.5) ].shape[0]\n",
    "    low = df[(df['id'].isin(dids)) & (df[\"0 - relevance - lower_pred\"]>=0.5) ].shape[0]\n",
    "    high = df[(df['id'].isin(dids)) & (df[\"0 - relevance - upper_pred\"]>=0.5) ].shape[0]\n",
    "    \n",
    "    table.loc[row.NAME,\"Documents\"] = f\"{mid} ({low}-{high})\"\n",
    "    \n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(198, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Documents</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IPCC Region</th>\n",
       "      <th>Impact</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">Alaska/N.W. Canada [ALA:1]</th>\n",
       "      <th>Terrestrial ES</th>\n",
       "      <td>1629 (1295-1942)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Coastal and marine Ecosystems</th>\n",
       "      <td>364 (254-476)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mountains, snow and ice</th>\n",
       "      <td>512 (403-609)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rivers, lakes, and soil moisture</th>\n",
       "      <td>441 (317-551)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Human and managed</th>\n",
       "      <td>183 (121-253)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>3961 (3363-4534)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Amazon [AMZ:7]</th>\n",
       "      <th>Terrestrial ES</th>\n",
       "      <td>52 (36-63)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Coastal and marine Ecosystems</th>\n",
       "      <td>313 (227-391)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mountains, snow and ice</th>\n",
       "      <td>30 (23-33)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rivers, lakes, and soil moisture</th>\n",
       "      <td>69 (37-101)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    Documents\n",
       "IPCC Region                Impact                                            \n",
       "Alaska/N.W. Canada [ALA:1] Terrestrial ES                    1629 (1295-1942)\n",
       "                           Coastal and marine Ecosystems        364 (254-476)\n",
       "                           Mountains, snow and ice              512 (403-609)\n",
       "                           Rivers, lakes, and soil moisture     441 (317-551)\n",
       "                           Human and managed                    183 (121-253)\n",
       "                           Total                             3961 (3363-4534)\n",
       "Amazon [AMZ:7]             Terrestrial ES                          52 (36-63)\n",
       "                           Coastal and marine Ecosystems        313 (227-391)\n",
       "                           Mountains, snow and ice                 30 (23-33)\n",
       "                           Rivers, lakes, and soil moisture       69 (37-101)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndf = pd.read_csv(\"../data/gridcell_studies_all_2.5.csv\")\n",
    "\n",
    "ndf.loc[ndf['LON']>180,\"LON\"]-=360\n",
    "ndf['ipccreg'] = 0\n",
    "\n",
    "\n",
    "index = pd.MultiIndex.from_product([ipcc_regions.NAME, [x.split(' - ')[1] for x in pred_cats]], names=['IPCC Region', 'Impact'])\n",
    "\n",
    "table = pd.DataFrame(columns=['Documents'], index=index)\n",
    "\n",
    "for i, row in ipcc_regions.iterrows():\n",
    "    inplace = shapely.vectorized.contains(row.geometry,ndf['LON'],ndf['LAT'])\n",
    "    idx = np.argwhere(inplace==True)\n",
    "    \n",
    "    ndf.loc[idx[:,0],\"ipccreg\"] = i+1\n",
    "    dids = place_gridcells[place_gridcells['ndf_id'].isin(idx)]['doc_id'].unique()\n",
    "    \n",
    "    for j, pc in enumerate(pred_cats):\n",
    "        \n",
    "        if \"Total\" in pc:\n",
    "            mid = df[(df['id'].isin(dids)) & (df[\"0 - relevance - mean_prediction\"]>=0.5) ].shape[0]\n",
    "            low = df[(df['id'].isin(dids)) & (df[\"0 - relevance - lower_pred\"]>=0.5) ].shape[0]\n",
    "            high = df[(df['id'].isin(dids)) & (df[\"0 - relevance - upper_pred\"]>=0.5) ].shape[0]            \n",
    "        else:\n",
    "            mid = df[\n",
    "                (df['id'].isin(dids)) & \n",
    "                (df[\"0 - relevance - mean_prediction\"]>=0.5) &\n",
    "                (df[pc] >= 0.5)\n",
    "            ].shape[0]\n",
    "            low = df[\n",
    "                (df['id'].isin(dids)) & \n",
    "                (df[\"0 - relevance - lower_pred\"]>=0.5) &\n",
    "                (df[pc.replace('mean_prediction','lower_pred')]>=0.5)\n",
    "            ].shape[0]\n",
    "            high = df[\n",
    "                (df['id'].isin(dids)) & \n",
    "                (df[\"0 - relevance - upper_pred\"]>=0.5) &\n",
    "                (df[pc.replace('mean_prediction','upper_pred')]>=0.5)\n",
    "            ].shape[0]\n",
    "    \n",
    "        table.loc[(row.NAME,pc.split(' - ')[1]),\"Documents\"] = f\"{mid} ({low}-{high})\"\n",
    "\n",
    "print(table.shape)\n",
    "table.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'da_trend_cat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/software/django-tmv/tmv/venv/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2888\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2889\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2890\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'da_trend_cat'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-edc4ca59c31a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m#D&A Trend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0msub_dadf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdadf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdadf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'da_trend_cat'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'DA>0.5'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mmid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msub_dadf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msub_dadf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmidids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mlow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msub_dadf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msub_dadf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlowids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/django-tmv/tmv/venv/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2899\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/django-tmv/tmv/venv/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2889\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2890\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2891\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2893\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'da_trend_cat'"
     ]
    }
   ],
   "source": [
    "ndf = pd.read_csv(\"../data/gridcell_studies_all_2.5.csv\")\n",
    "\n",
    "ndf.loc[ndf['LON']>180,\"LON\"]-=360\n",
    "ndf['ipccreg'] = 0\n",
    "\n",
    "#df = pd.DataFrame(columns=['IPCC region', 'Documents'])\n",
    "index = pd.Index(ipcc_regions.NAME, name=\"IPCC Region\")\n",
    "\n",
    "table = pd.DataFrame(columns=['D&A Trend', \"nonD&A Trend\", \"NAD&A Trend\", \"Sensitivity\",\"Detection\"], index=index)\n",
    "\n",
    "for i, row in ipcc_regions.iterrows():\n",
    "    inplace = shapely.vectorized.contains(row.geometry,ndf['LON'],ndf['LAT'])\n",
    "    idx = np.argwhere(inplace==True)\n",
    "    \n",
    "    ndf.loc[idx[:,0],\"ipccreg\"] = i+1\n",
    "    dids = place_gridcells[place_gridcells['ndf_id'].isin(idx)]['doc_id'].unique()\n",
    "    \n",
    "    midids = df[(df['id'].isin(dids)) & (df[\"0 - relevance - mean_prediction\"]>=0.5) ]['id']\n",
    "    lowids = df[(df['id'].isin(dids)) & (df[\"0 - relevance - lower_pred\"]>=0.5) ]['id']\n",
    "    highids = df[(df['id'].isin(dids)) & (df[\"0 - relevance - upper_pred\"]>=0.5) ]['id']\n",
    "    \n",
    "    #D&A Trend\n",
    "    sub_dadf = dadf[dadf['da_trend_cat']=='DA>0.5']\n",
    "    mid = sub_dadf[sub_dadf['id'].isin(midids)].shape[0]\n",
    "    low = sub_dadf[sub_dadf['id'].isin(lowids)].shape[0]\n",
    "    high = sub_dadf[sub_dadf['id'].isin(highids)].shape[0]\n",
    "    \n",
    "    table.loc[row.NAME,\"D&A Trend\"] = f\"{mid} ({low}-{high})\"\n",
    "    \n",
    "    #NO D&A Trend\n",
    "    sub_dadf = dadf[dadf['da_trend_cat']=='0==DA']\n",
    "    mid = sub_dadf[sub_dadf['id'].isin(midids)].shape[0]\n",
    "    low = sub_dadf[sub_dadf['id'].isin(lowids)].shape[0]\n",
    "    high = sub_dadf[sub_dadf['id'].isin(highids)].shape[0]\n",
    "    \n",
    "    table.loc[row.NAME,\"nonD&A Trend\"] = f\"{mid} ({low}-{high})\"\n",
    "    \n",
    "    #NO D&A Trend\n",
    "    sub_dadf = dadf[dadf['da_trend_cat']=='NA']\n",
    "    mid = sub_dadf[sub_dadf['id'].isin(midids)].shape[0]\n",
    "    low = sub_dadf[sub_dadf['id'].isin(lowids)].shape[0]\n",
    "    high = sub_dadf[sub_dadf['id'].isin(highids)].shape[0]\n",
    "    \n",
    "    table.loc[row.NAME,\"NAD&A Trend\"] = f\"{mid} ({low}-{high})\"\n",
    "    \n",
    "    #NO D&A Trend\n",
    "    sub_dadf = dadf[dadf['da_trend_cat']=='NA']\n",
    "    mid = sub_dadf[sub_dadf['id'].isin(midids)].shape[0]\n",
    "    low = sub_dadf[sub_dadf['id'].isin(lowids)].shape[0]\n",
    "    high = sub_dadf[sub_dadf['id'].isin(highids)].shape[0]\n",
    "    \n",
    "    table.loc[row.NAME,\"NAD&A Trend\"] = f\"{mid} ({low}-{high})\"\n",
    "    \n",
    "    #break\n",
    "    \n",
    "table.head(10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dadf.da_trend_cat.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf = pd.read_csv(\"../data/gridcell_studies_all_2.5.csv\")\n",
    "ndf[ndf['index']==7574].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_gridcells = pd.read_csv(\"../data/study_gridcell_all_2.5.csv\")\n",
    "place_gridcells.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipcc_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_cat_df = pd.read_csv('../data/1_predicted_category_documents_specific.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = df.merge(extra_cat_df)\n",
    "\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_impact_cats = [x for x in merged_df.columns if \"18 -\" in x and \"mean\" in x]\n",
    "\n",
    "specific_impact_cats = [\n",
    "    '18 - Food/Agriculture - mean_prediction',\n",
    "    '18 - Livelihoods and wellbeing - mean_prediction',\n",
    "    '18 - Health - mean_prediction',\n",
    "    '18 - Displacement and migration - mean_prediction',\n",
    "]\n",
    "\n",
    "for c in specific_impact_cats:\n",
    "    print(merged_df[c].sum())\n",
    "    label = c.replace(\" - mean_prediction\",'')\n",
    "    cs = [c, c.replace('mean_prediction','lower_pred'), c.replace('mean_prediction','upper_pred')]\n",
    "    merged_df.loc[merged_df[label]==1,cs] = 1\n",
    "    print(merged_df[c].sum())\n",
    "print(pred_cats)\n",
    "\n",
    "specific_impact_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycountry_convert import country_name_to_country_alpha3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_dict = []\n",
    "for x in \"\"\"Algeria, Angola, Benin, Botswana, Burkina Faso, Burundi,  Cameroon, Chad, Congo, Ivory Coast, Djibouti, Egypt, Eritrea, Eswatini, Swaziland, Ethiopia, Gabon, Gambia, Ghana, Guinea, Kenya, Lesotho, Liberia, Libya, Malawi, Mali, Mauritania, Morocco, Mozambique, Namibia, Niger, Nigeria, Rwanda, Senegal, Sierra Leone, Somalia, Sudan, Tanzania, Togo, Tunisia, Uganda, Zambia, Zimbabwe\"\"\".split(', '):\n",
    "    country_dict.append({\"country\": country_name_to_country_alpha3(x.strip()), \"region\": \"Africa\"})\n",
    "    \n",
    "for x in \"\"\"Afghanistan, Armenia, Azerbaijan, Bahrain, Bangladesh, Bhutan, Brunei, Cambodia, China, Cyprus, Georgia, India, Indonesia, Iran, Iraq, Israel, Japan, Jordan, Kazakhstan, Kuwait, Kyrgyzstan, Laos, Lebanon, Malaysia, Mongolia, Myanmar, Nepal, South Korea, North Korea, Oman, Pakistan, Palestine, Philippines, Qatar, Russia, Saudi Arabia, Singapore, Sri Lanka, Syria, Taiwan, Tajikistan, Thailand, Turkey, Turkmenistan, United Arab Emirates, Uzbekistan, Vietnam, Yemen\n",
    "\"\"\".split(', '):\n",
    "    country_dict.append({\"country\": country_name_to_country_alpha3(x.strip()), \"region\": \"Asia\"})\n",
    "    \n",
    "for x in \"\"\"Australia,  New Zealand\"\"\".split(', '):\n",
    "    country_dict.append({\"country\": country_name_to_country_alpha3(x.strip()), \"region\": \"Australasia\"})\n",
    "    \n",
    "for x in \"\"\"Belize, Costa Rica, El Salvador, Guatemala, Honduras, Nicaragua, Panama, Argentina, Bolivia, Brazil, Chile, Colombia, Ecuador, French Guiana, Guyana, Paraguay, Peru, Suriname, Uruguay, Venezuela\n",
    "\"\"\".split(', '):\n",
    "    country_dict.append({\"country\": country_name_to_country_alpha3(x.strip()), \"region\": \"Central and South America\"})\n",
    "    \n",
    "for x in \"\"\"United States, Canada, Mexico, Greenland\n",
    "\"\"\".split(', '):\n",
    "    country_dict.append({\"country\": country_name_to_country_alpha3(x.strip()), \"region\": \"North America\"})\n",
    "    \n",
    "for x in \"\"\"Albania, Andorra, Armenia, Austria, Azerbaijan, Belarus, Belgium, Bosnia and Herzegovina, Bulgaria, Croatia, Cyprus, Czech Republic, Denmark, Estonia, Finland, France, Georgia, Germany, Greece, Hungary, Iceland, Ireland, Italy, Kazakhstan, Kosovo, Latvia, Liechtenstein, Lithuania, Luxembourg, Malta, Moldova, Monaco, Montenegro, Netherlands, Macedonia, Norway, Poland, Portugal, Romania, Russia, San Marino, Serbia, Slovakia, Slovenia, Spain, Sweden, Switzerland, Turkey, Ukraine, United Kingdom, Vatican City\n",
    "\"\"\".split(', '):\n",
    "    try:\n",
    "        country_dict.append({\"country\": country_name_to_country_alpha3(x.strip()), \"region\": \"Europe\"})\n",
    "    except:\n",
    "        if x.strip()==\"Kosovo\":\n",
    "            country_dict.append({\"country\": \"XKX\", \"region\": \"Europe\"})\n",
    "        elif x.strip()==\"Vatican City\":\n",
    "            country_dict.append({\"country\": \"VAT\", \"region\": \"Europe\"})\n",
    "    \n",
    "for x in \"\"\"Anguilla, Aruba, Antigua and Barbuda, Bahamas, Bahrain, Barbados, Bermuda, British Virgin Islands, Cayman Islands, Northern Mariana Islands, Belize, Comoros, Cuba, Dominica, Grenada, Guyana, Haiti, Jamaica, Saint Kitts and Nevis, Saint Lucia, Saint Vincent and the Grenadines, Suriname, Trinidad and Tobago, Cabo Verde, Curaçao, Comoros, Guinea, Maldives, Mauritius, São Tomé and Príncipe, Seychelles, Singapore, Cook Islands, Fiji, Kiribati, Marshall Islands, Micronesia, Nauru, Niue, Palau, Samoa, Solomon Islands, Seychelles, East Timor, Tonga, Tuvalu, Vanuatu, French Polynesia, Guadeloupe, Guam, Martinique, Montserrat, New Caledonia, Puerto Rico, Saint Martin, Turks and Caicos, U.S. Virgin Islands, Guinea-Bissau, Cabo Verde, Comoros, Madagascar, Mauritius, Sao Tome and Principe, Seychelles\n",
    "\"\"\".split(', '):\n",
    "    try:\n",
    "        country_dict.append({\"country\": country_name_to_country_alpha3(x.strip()), \"region\": \"Small Island States\"})\n",
    "    except:\n",
    "        if x.strip()==\"U.S. Virgin Islands\":\n",
    "            country_dict.append({\"country\": \"VIR\", \"region\": \"Small Island States\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_df = pd.DataFrame.from_dict(country_dict)\n",
    "regions = region_df.region.unique()\n",
    "region_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places = pd.read_csv('../data/place_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pd.MultiIndex.from_product([specific_impact_cats, [\"Partially attributed\",\"Not attributed\"]], names=['IPCC Region', 'Impact'])\n",
    "\n",
    "table = pd.DataFrame(columns=regions, index=index)\n",
    "\n",
    "for region in regions:\n",
    "    countries = region_df.loc[region_df['region']==region,\"country\"]\n",
    "    place_ids = places.loc[places['country_predicted'].isin(countries),\"doc_id\"]\n",
    "    for impact in specific_impact_cats:\n",
    "        sub_df = merged_df.loc[\n",
    "            (merged_df['id'].isin(place_ids)) & \n",
    "            (merged_df[impact]>0.5)\n",
    "        ]\n",
    "        table.loc[(impact,\"Partially attributed\"),region] = sub_df[sub_df[\"da\"]>0].shape[0]\n",
    "        table.loc[(impact,\"Not attributed\"),region] = sub_df.shape[0] - sub_df[sub_df[\"da\"]>0].shape[0]\n",
    "\n",
    "                                    \n",
    "table.head(10)\n",
    "table.to_excel('../data/human_regions.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmv",
   "language": "python",
   "name": "tmv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
