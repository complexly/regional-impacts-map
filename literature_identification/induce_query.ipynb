{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import django, sys, os\n",
    "sys.path.append('/home/max/software/django-tmv/tmv_mcc-apsis/BasicBrowser')\n",
    "os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"BasicBrowser.settings\")\n",
    "django.setup()\n",
    "\n",
    "from scoping.models import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from itertools import product, combinations\n",
    "from utils.text import *\n",
    "from scipy.sparse import find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = Doc.objects.filter(query=6956, content__iregex=\"\\w\").order_by('id')\n",
    "docs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<232x177 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2034 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords as sw\n",
    "stopwords = set(sw.words('english'))\n",
    "\n",
    "stopwords | set([\"use\"])\n",
    "\n",
    "vec = CountVectorizer(\n",
    "    ngram_range=(1,2),\n",
    "    min_df=5, strip_accents='unicode', \n",
    "    max_features=10000,\n",
    "    stop_words=stopwords,\n",
    "    tokenizer=snowball_stemmer()\n",
    ")\n",
    "\n",
    "#X = vec.fit_transform(docs.values_list('content',flat=True))\n",
    "def mash_texts(x):\n",
    "    s = \"\"\n",
    "    for at in [\"de\", \"kwp\"]:#,\"ab\", \"ti\",]:\n",
    "        if hasattr(x.wosarticle, at) and getattr(x.wosarticle, at):\n",
    "            s+=f\" {getattr(x.wosarticle, at)}\"\n",
    "    return s.strip()\n",
    "texts = [mash_texts(x) for x in docs]\n",
    "X = X = vec.fit_transform(texts)\n",
    "vocab = vec.get_feature_names()\n",
    "X[X.nonzero()] = 1\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chang climat', 'climat chang', 'climat variabl', 'climatechang massbal', 'coral reef', 'global chang', 'global warm', 'ice cap', 'mountain glacier', 'rang shift', 'regim shift', 'remot sens', 'respons impact', 'sealevel rise', 'seasurfac temperatur']\n"
     ]
    }
   ],
   "source": [
    "print([v for v in vocab if \" \" in v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices, vals = np.array(X.sum(0))[0].argsort()[::-1], np.sort(np.array(X.sum(0))[0])[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alpine plants; Argyroxyphium sandwicense; biodiversity loss; climate change ecology; hotspot; population declines; silversword SILVERSWORD ALLIANCE COMPOSITAE; INDUCED TREE MORTALITY; GIANT ROSETTE PLANT; DROUGHT; ALPINE; TEMPERATURE; EXTINCTION; MOUNTAIN; MADIINAE; FOREST'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"climat*\" OR \"climatechang*\" OR \"temperatur*\" OR \"impact*\" OR \"shift*\" OR \"chang*\" OR \"drought*\" OR \"island*\" OR \"veget*\" OR \"glacier*\" OR \"conserv*\" OR \"massbal*\" OR \"popul*\" OR \"ecolog*\" OR \"sens*\" OR \"biodivers*\" OR \"rang*\" OR \"precipit*\" OR \"river*\" OR \"mountain*\" returns 214\n"
     ]
    }
   ],
   "source": [
    "# get the combination of relevant words that return all documents\n",
    "exclusions = []\n",
    "stop = [\n",
    "    \"increas\",\"use\",\"data\",\"studi\",\"result\",\"global\",\"variabl\",\"effect\",\"show\",\"model\",\"current\",\n",
    "    \"year\",\"respons\",\"also\",\"observ\",\"relat\",\"level\",\"signific\",\"global warm\",\"trend\",\"growth\",\n",
    "    \"simul\", \"sea\", \"ecosystem\", \"marin\",\"forest\",\"product\",\"water\"\n",
    "]\n",
    "\n",
    "for j in range(1):\n",
    "    ids = []\n",
    "    for i in range(20):\n",
    "        if i==0:\n",
    "            for ind in indices:\n",
    "                w = vocab[ind]\n",
    "                if w in stop:\n",
    "                    continue\n",
    "                if ind not in exclusions and ind not in ids:\n",
    "                    x = X[:,ind]\n",
    "                    ids.append(ind)\n",
    "                    exclusions.append(ind)\n",
    "                    break\n",
    "            continue\n",
    "        max_x = len(x.data)\n",
    "        max_ind = None\n",
    "        for ind in indices:\n",
    "            w = vocab[ind]\n",
    "            if w in stop:\n",
    "                continue\n",
    "            if ind in ids or ind in exclusions:\n",
    "                continue\n",
    "            tmp_x = x + X[:,ind]\n",
    "            if len(tmp_x.data) > max_x:\n",
    "                max_x = len(tmp_x.data)\n",
    "                max_ind = ind\n",
    "        if not max_ind:\n",
    "            break\n",
    "        else:\n",
    "            x = x + X[:,max_ind]\n",
    "            ids.append(max_ind)\n",
    "\n",
    "    strings = [f'\"{vocab[x]}*\"' for x in ids]\n",
    "    print(f'{\" OR \".join(strings)} returns {len(x.data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = X[:,1]+X[:,2]\n",
    "#l = lx[x > 1].shape[1]\n",
    "\n",
    "x[x==1] = 0\n",
    "x.eliminate_zeros()\n",
    "x[x>0] = 1\n",
    "x.getnnz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['climat', 'climatechang', 'temperatur', 'impact', 'shift', 'chang', 'drought', 'island', 'veget', 'glacier', 'conserv', 'massbal', 'popul', 'ecolog', 'sens', 'biodivers', 'rang', 'precipit', 'river', 'mountain']\n"
     ]
    }
   ],
   "source": [
    "kws = [vocab[x] for x in ids]\n",
    "print(kws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<232x618 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 16082 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X = vec.fit_transform(docs.values_list('content',flat=True))\n",
    "\n",
    "vec = CountVectorizer(\n",
    "    ngram_range=(1,2),\n",
    "    min_df=10, strip_accents='unicode', \n",
    "    max_features=10000,\n",
    "    stop_words=stopwords,\n",
    "    tokenizer=snowball_stemmer()\n",
    ")\n",
    "\n",
    "def mash_texts(x):\n",
    "    s = \"\"\n",
    "    for at in [\"de\", \"kwp\",\"ab\", \"ti\",]:\n",
    "        if hasattr(x.wosarticle, at) and getattr(x.wosarticle, at):\n",
    "            s+=f\" {getattr(x.wosarticle, at)}\"\n",
    "    return s.strip()\n",
    "texts = [mash_texts(x) for x in docs]\n",
    "X = X = vec.fit_transform(texts)\n",
    "vocab = vec.get_feature_names()\n",
    "X[X.nonzero()] = 1\n",
    "X\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only Climate keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"climat*\" OR \"region*\" OR \"impact*\" OR \"occur*\" OR \"caus*\" OR \"may*\" OR \"temperatur*\" returns 232\n"
     ]
    }
   ],
   "source": [
    "# get the combination of relevant words that return all documents\n",
    "indices, vals = np.array(X.sum(0))[0].argsort()[::-1], np.sort(np.array(X.sum(0))[0])[::-1]\n",
    "\n",
    "exclusions = []\n",
    "stop = [\n",
    "    \"increas\",\"use\",\"data\",\"studi\",\"result\",\"global\",\"variabl\",\"effect\",\"show\",\"model\",\"current\",\n",
    "    \"year\",\"respons\",\"also\",\"observ\",\"relat\",\"level\",\"signific\",\"trend\",\"growth\",\n",
    "    \"simul\", \"sea\", \"ecosystem\", \"marin\",\"forest\",\"product\",\"water\",'alter','alp','although','consist',\n",
    "    'amount','chang','area','dur','dynam','becom'\n",
    "]\n",
    "\n",
    "for j in range(1):\n",
    "    ids = []\n",
    "    for i in range(20):\n",
    "        if i==0:\n",
    "            for ind in indices:\n",
    "                w = vocab[ind]\n",
    "                if w in stop:\n",
    "                    continue\n",
    "                if ind not in exclusions and ind not in ids:\n",
    "                    x = X[:,ind]\n",
    "                    ids.append(ind)\n",
    "                    exclusions.append(ind)\n",
    "                    break\n",
    "            continue\n",
    "        max_x = len(x.data)\n",
    "        max_ind = None\n",
    "        for ind in indices:\n",
    "            w = vocab[ind]\n",
    "            if w in stop:\n",
    "                continue\n",
    "            if ind in ids or ind in exclusions:\n",
    "                continue\n",
    "            tmp_x = x + X[:,ind]\n",
    "            if len(tmp_x.data) > max_x:\n",
    "                max_x = len(tmp_x.data)\n",
    "                max_ind = ind\n",
    "        if not max_ind:\n",
    "            break\n",
    "        else:\n",
    "            x = x + X[:,max_ind]\n",
    "            ids.append(max_ind)\n",
    "\n",
    "    strings = [f'\"{vocab[x]}*\"' for x in ids]\n",
    "    print(f'{\" OR \".join(strings)} returns {len(x.data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TS=(climate model OR elevated* temperatur OR ocean* warming OR saline* intrusion OR chang* climat OR environment* change OR climat* change, OR climat* warming OR warming* climat OR climat* varia OR global* warming OR greenhouse* effect OR anthropogen* OR sea* level OR precipitation variabil OR temperature* impact OR environmental* variab OR weather* pattern OR weather* factor*) OR TS=(change* NEAR/5  cryosphere)\n",
      "starting point: 220 docs\n",
      "217\n",
      "\n",
      "#####\n",
      "1461495 The Major Floods in the Amazonas River and Tributaries (Western Amazon Basin) during the 1970-2012 Period: A Focus on the 2012 Flood\n",
      "<QuerySet [<DocAuthInst: Espinoza, JC>, <DocAuthInst: Espinoza, JC>, <DocAuthInst: Ronchail, J>, <DocAuthInst: Ronchail, J>, <DocAuthInst: Frappart, F>, <DocAuthInst: Frappart, F>, <DocAuthInst: Lavado, W>, <DocAuthInst: Lavado, W>, <DocAuthInst: Santini, W>, <DocAuthInst: Santini, W>, <DocAuthInst: Guyot, JL>, <DocAuthInst: Guyot, JL>]>\n",
      "2013\n",
      "In this work, the authors analyze the origin of the extreme floods in the Peruvian Amazonas River during the 1970-2012 period, focusing on the recent April 2012 flooding (55 400 m(3) s(-1)). Several hydrological variables, such as rainfall, terrestrial water storage, and discharge, point out that the unprecedented 2012 flood is mainly related to an early and abundant wet season over the north of the basin. Thus, the peak of the Maranon River, the northern contributor of the Amazonas, occurred sooner than usual (in April instead of May), coinciding with the peak of the Ucayali River, the southern contributor. This concomitance caused a dramatic flood downstream in the Peruvian Amazonas. These results are compared to the amplitude and timing of the three most severe extreme floods (1970-2011). The analysis of the climatic features related to the most important floods (1986, 1993, 1999, and 2012) suggests that they are characterized by a La Nina event, which originates a geopotential height wave train near the ground, with positive anomalies over the subtropical South and North Pacific and Atlantic and over southeastern South America. These patterns contribute to 1) the origin of an abundant humidity transport flux from the tropical North Atlantic and the Caribbean Sea toward the northwestern Amazon and 2) the maintenance of the monsoon flux over this region. They both favor a strong convergence of humidity in the northern Amazonas basin. Finally, the authors suggest that the intensity of floods is more likely related to an early La Nina event (as observed during the 2011/12 season), early rainfall, and simultaneous peaks of both tributaries of the Amazonas River.\n",
      "None\n",
      "DROUGHT\n",
      "\n",
      "#####\n",
      "2089982 Quantitative remote sensing study indicates doubling of coastal erosion rate in past 50 yr along a segment of the Arctic coast of Alaska\n",
      "<QuerySet [<DocAuthInst: Mars, JC>, <DocAuthInst: Houseknecht, DW>]>\n",
      "2007\n",
      "A new quantitative coastal land gained-and-lost method uses image analysis of topographic maps and Landsat thematic mapper short-wave infrared data to document accelerated coastal land loss and thermokarst lake expansion and drainage. The data span 1955-2005 along the Beaufort Sea coast north of Teshekpuk Lake in the National Petroleum Reserve in Alaska. Some areas have undergone as much as 0.9 kin of coastal erosion in the past 50 yr. Land loss attributed to coastal erosion more than doubled, from 0.48 km(2) yr(-1) during 1955-1985 to 1.08 km(2) yr(-1) during 1985-2005. Coastal erosion has breached thermokarst lakes, causing initial draining of the lakes followed by marine flooding. Although inland thermokarst lakes show some uniform expansion, lakes breached by coastal erosion display lake expansion several orders of magnitude greater than inland lakes.\n",
      "remote sensing; landsat thematic mapper; Teshekpuk lake; coastal erosion; permafrost; Alaska; Arctic; national petroleum reserve; erosion.\n",
      "BEAUFORT SEA COAST\n",
      "\n",
      "#####\n",
      "2090013 The arrival, establishment and spread of exotic diseases: patterns and predictions\n",
      "<QuerySet [<DocAuthInst: Randolph, SE>, <DocAuthInst: Rogers, DJ>]>\n",
      "2010\n",
      "The impact of human activities on the principles and processes governing the arrival, establishment and spread of exotic pathogens is illustrated by vector-borne diseases such as malaria, dengue, chikungunya, West Nile, bluetongue and Crimean-Congo haemorrhagic fevers. Competent vectors, which are commonly already present in the areas, provide opportunities for infection by exotic pathogens that are introduced by travel and trade. At the same time, the correct combination of environmental conditions (both abiotic and biotic) makes many far-flung parts of the world latently and predictably, but differentially, permissive to persistent transmission cycles. Socioeconomic factors and nutritional status determine human exposure to disease and resistance to infection, respectively, so that disease incidence can vary independently of biological cycles.\n",
      "None\n",
      "WEST-NILE-VIRUS; CONGO HEMORRHAGIC-FEVER; TICK-BORNE ENCEPHALITIS; NEW-YORK-CITY; AFRICAN HORSE SICKNESS; AEDES-ALBOPICTUS; CHIKUNGUNYA-VIRUS; BLUETONGUE-VIRUS; SPATIAL-DISTRIBUTION; INFECTIOUS-DISEASES\n"
     ]
    }
   ],
   "source": [
    "pats = [\n",
    "    \"climate model\",\n",
    "    \"elevated\\w* temperatur\",\n",
    "    \"ocean\\w* warming\",\n",
    "    \"saline\\w* intrusion\",\n",
    "    \"chang\\w* climat\",\n",
    "    \"environment\\w* change\",\n",
    "    \"climat\\w* change,\",\n",
    "    \"climat\\w* warm\",\n",
    "    \"warming\\w* climat\",\n",
    "    \"climat\\w* varia\",\n",
    "    \"global\\w* warming\",\n",
    "    \"greenhouse\\w* effect\",\n",
    "    \"anthropogen\\w*\",\n",
    "    \"sea\\w* level\",\n",
    "    \"precipitation variabil\",\n",
    "    \"temperature\\w* impact\",\n",
    "    \"environmental\\w* variab\",\n",
    "    \"change\\w* (\\w+\\s*\\W*){5} cryosphere\",\n",
    "    \"weather\\w* pattern\",\n",
    "    \"weather\\w* factor\\w*\",\n",
    "]\n",
    "rpat = \"|\".join([x.replace(\" \",\"( |-)\") for x in pats])\n",
    "\n",
    "#\" OR \".join([x.replace('\\w*','*').replace('(\\w+\\s*\\W*){','NEAR/').replace('}','') for x in pats])\n",
    "opats = \" OR \".join([x.replace('\\w*','*') for x in pats if '(\\w+\\s*\\W*){' not in x])\n",
    "npats = \" OR \".join([x.replace('\\w*','*').replace('(\\w+\\s*\\W*){','NEAR/').replace('}',' ') for x in pats if '(\\w+\\s*\\W*){' in x])\n",
    "    \n",
    "wpat = f\"TS=({opats}) OR TS=({npats})\"\n",
    "\n",
    "print(wpat)\n",
    "\n",
    "#for s in pats:\n",
    "    #if \n",
    "# Climate in keywords\n",
    "\n",
    "## WHY IS OLIVEIRA, 2007 included?\n",
    "### Also, Nepstad and Stickler, 2008\n",
    "### Polidoro 2010, very weak - not main conclusion\n",
    "\n",
    "# Exclude non-climate docs\n",
    "docs = docs.exclude(pk__in=[\n",
    "    1622783, # Why is Oliveira, 2007 included?\n",
    "    1627701, # Also, Nepstad and Stickler, 2008\n",
    "    2090005, # Nepstad, 2006\n",
    "    2089960, # Wassenaar, 2007 - LUC\n",
    "    1627733, # Polidoro 2010, very weak - not main conclusion\n",
    "    2090004, # Veran 2007 - long line fishing\n",
    "    2089967, # Giri - mangrove database, climate only mentioned in conclusion and with reference\n",
    "    2089944, # Jellyman - eels\n",
    "    2091676, # Novelo-Casanova, 2010: Cayman Islands climate??\n",
    "    2089959, # Bruno, 2007 \n",
    "    2091680, # Guzman, 2008, we consider the main anthropogenic threats to the coral reefs and communities of the islands are overfishing, sedimentation and tourism\n",
    "    2089950, # Razumov - permafrost and erosion - climate? is the ref there as counter-evidence Nevertheless, the longterm average annual rate\n",
    "#                             of erosion of the coast line of the bottom of the Arctic\n",
    "#                             Seas is much lower than in the seas outside the cry\n",
    "#                             olithozone even under the conditions of the climate’s\n",
    "#                             warming accompanied by the activation of thermo\n",
    "#                             abrasion and during the subaqual abrasion processes\n",
    "#                             (Fig. 5). In particular, this proves the development of\n",
    "#                             ordinary abrasionaccumulation processes in the Arc\n",
    "#                             tic Seas, and, here, we cannot say about the influence\n",
    "#                             of thermal slumps on the formation of the underwater\n",
    "#                             slope relief\n",
    "])\n",
    "\n",
    "#\n",
    "\n",
    "print(f\"starting point: {docs.count()} docs\")\n",
    "\n",
    "\n",
    "\n",
    "tmatch = docs.filter(title__iregex=rpat).values_list('pk',flat=True)\n",
    "abmatch = docs.filter(content__iregex=rpat).values_list('pk',flat=True)\n",
    "dematch = docs.filter(wosarticle__de__iregex=rpat).values_list('pk',flat=True)\n",
    "kwpmatch = docs.filter(wosarticle__kwp__iregex=rpat).values_list('pk',flat=True)\n",
    "\n",
    "declim = docs.filter(wosarticle__de__icontains=\"climat\").values_list('pk',flat=True)\n",
    "kwpclim = docs.filter(wosarticle__kwp__icontains=\"climat\").values_list('pk', flat=True)\n",
    "ticlim = docs.filter(title__icontains=\"climat\").values_list('pk', flat=True)\n",
    "\n",
    "mids = set(tmatch) | set(abmatch) | set(dematch) | set(kwpmatch) | set(declim) | set(kwpclim) | set(ticlim)\n",
    "\n",
    "mdocs = docs.filter(pk__in=mids)\n",
    "\n",
    "print(mdocs.count())\n",
    "\n",
    "nmdocs=docs.exclude(pk__in=mids)\n",
    "\n",
    "for d in nmdocs[:10]:\n",
    "    print('\\n#####')\n",
    "    print(d.id, d.title)\n",
    "    print(d.docauthinst_set.all().order_by('position'))\n",
    "    print(d.PY)\n",
    "    print(d.content)\n",
    "    print(d.wosarticle.de)\n",
    "    print(d.wosarticle.kwp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['climate',\n",
       " 'model',\n",
       " 'elevated',\n",
       " 'temperatur',\n",
       " 'ocean',\n",
       " 'warming',\n",
       " 'saline',\n",
       " 'intrusion',\n",
       " 'chang',\n",
       " 'climat',\n",
       " 'environment',\n",
       " 'change',\n",
       " 'climat',\n",
       " 'change,',\n",
       " 'climat',\n",
       " 'warming',\n",
       " 'warming',\n",
       " 'climat',\n",
       " 'climat',\n",
       " 'varia',\n",
       " 'global',\n",
       " 'warming',\n",
       " 'greenhouse',\n",
       " 'effect',\n",
       " 'anthropogen',\n",
       " 'sea',\n",
       " 'level',\n",
       " 'precipitation',\n",
       " 'variabil',\n",
       " 'temperature',\n",
       " 'impact',\n",
       " 'environmental',\n",
       " 'variab',\n",
       " 'change',\n",
       " 'cryosphere',\n",
       " 'weather',\n",
       " 'pattern',\n",
       " 'weather',\n",
       " 'factor']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.utils import flatten\n",
    "\n",
    "climate_words = list(flatten([[y.replace('\\w*','') for y in x.split() if re.match('\\w{2,}',y)] for x in pats]))\n",
    "\n",
    "climate_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [mash_texts(x) for x in mdocs]\n",
    "X = X = vec.fit_transform(texts)\n",
    "vocab = vec.get_feature_names()\n",
    "X[X.nonzero()] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44850\n",
      "37125\n"
     ]
    }
   ],
   "source": [
    "# Find combinations of vocab (except for those that contain the stopwords below)\n",
    "\n",
    "indices, vals = np.array(X.sum(0))[0].argsort()[::-1], np.sort(np.array(X.sum(0))[0])[::-1]\n",
    "\n",
    "all_cs = list(combinations(indices[:300],2))\n",
    "\n",
    "print(len(all_cs))\n",
    "\n",
    "c_lengths = []\n",
    "cs = []\n",
    "max_x = 0\n",
    "max_ind = 0\n",
    "local_stops = set([\n",
    "    \"use\",\"also\",\"studi\",\"may\",\"larg\",\"dure\",\"occur\",\"result\",\n",
    "    \"climat chang\",\"year\"\n",
    "]) | set(words)\n",
    "\n",
    "stop_combos = [\n",
    "    [\"studi\",\"year\"],\n",
    "    [\"year\",\"data\"],\n",
    "    [\"year\",\"observ\"],\n",
    "    [\"year\",\"dure\"],\n",
    "    [\"data\",\"indic\"],\n",
    "    [\"water\",\"ocean\"],\n",
    "    [\"increase\", \"result\"],\n",
    "    [\"sea\",\"ocean\"],\n",
    "    [\"chang\", \"increas\"],\n",
    "    [\"increas\", \"temperatur\"],\n",
    "    [\"data\", \"analysi\"],\n",
    "    [\"impact\",\"effect\"],\n",
    "    [\"result\", \"high\"],\n",
    "    [\"chang\",\"warm\"],\n",
    "    [\"climat\",\"warm\"],\n",
    "    [\"climat\",\"temperatur\"],\n",
    "    [\"show\", \"data\"]\n",
    "]\n",
    "\n",
    "for c in all_cs:\n",
    "    words = [vocab[x] for x in c]\n",
    "    if len(local_stops & set(words))>0:\n",
    "        continue\n",
    "    if words in stop_combos:\n",
    "        continue\n",
    "    if len(set(words) & set(climate_words)) > 0:\n",
    "        continue\n",
    "    x = sum([X[:,x] for x in c])\n",
    "    l = x[x > 1].shape[1]\n",
    "    c_lengths.append(l)   \n",
    "    cs.append(c)\n",
    "    \n",
    "print(len(cs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: 101\n",
      "3: 127\n",
      "4: 144\n",
      "5: 157\n",
      "6: 169\n",
      "7: 179\n",
      "8: 188\n",
      "9: 194\n",
      "10: 199\n",
      "11: 203\n",
      "12: 207\n",
      "13: 210\n",
      "14: 213\n",
      "15: 215\n",
      "16: 217\n",
      "['increas* AND region*', 'warm* AND respons*', 'increas* AND trend*', 'period* AND area*', 'show* AND relat*', 'speci* AND ecosystem*', 'increas* AND data*', 'variabl* AND mean*', 'water* AND surfac*', 'warm* AND associ*', 'region* AND data*', 'speci* AND shift*', 'increas* AND scale*', 'ice* AND glacier*', 'signific* AND time*', 'rate* AND develop*']\n",
      "217\n"
     ]
    }
   ],
   "source": [
    "ids = []\n",
    "\n",
    "# Go through the combinations, choosing the one that adds the most relevant documents\n",
    "seen = []\n",
    "for i in range(25):\n",
    "    \n",
    "    if i==0:\n",
    "        c = cs[np.argsort(c_lengths)[::-1][0]]\n",
    "        x = sum([X[:,xi] for xi in c])\n",
    "        x[x==1] = 0\n",
    "        x.eliminate_zeros()\n",
    "        x[x>0] = 1\n",
    "        ids.append(c)\n",
    "        continue\n",
    "\n",
    "    base_l = len(x.data)\n",
    "    max_l = len(x.data)\n",
    "    max_ind = None\n",
    "    \n",
    "    for ind in np.argsort(c_lengths)[::-1]:\n",
    "        if ind in seen:\n",
    "            continue\n",
    "        c = cs[ind]\n",
    "        words = [vocab[x] for x in c]\n",
    "        if len(local_stops & set(words))>0:\n",
    "            continue\n",
    "        if words in stop_combos:\n",
    "            continue\n",
    "        if c in ids:\n",
    "            continue\n",
    "        tmp_x = sum([X[:,xi] for xi in c])\n",
    "        for w in range(1, len(c)):\n",
    "            tmp_x[tmp_x==w] = 0\n",
    "        tmp_x.eliminate_zeros()\n",
    "        tmp_x[tmp_x>0] = 1\n",
    "        tmp_x = x + tmp_x\n",
    "        l = len(tmp_x.data)\n",
    "        if l > max_l:\n",
    "            max_l = l\n",
    "            max_ind = ind\n",
    "        if l <= base_l: \n",
    "            seen.append(ind)\n",
    "            \n",
    "    if not max_ind:\n",
    "        break\n",
    "    else:\n",
    "        c = cs[max_ind]\n",
    "        tmp_x = sum([X[:,xi] for xi in c])\n",
    "        for w in range(1, len(c)):\n",
    "            tmp_x[tmp_x==w] = 0\n",
    "        tmp_x.eliminate_zeros()\n",
    "        tmp_x[tmp_x>0] = 1\n",
    "        x = x + tmp_x\n",
    "        x[x>1]=1\n",
    "        ids.append(c)\n",
    "        print(f\"{i+1}: {max_l}\")\n",
    "        if x.shape[0] == max_l:\n",
    "            break\n",
    "                \n",
    "c = [\" AND \".join([vocab[x]+\"*\" for x in c]) for c in ids]\n",
    "print(c)\n",
    "print(max_l)       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TS=((climat* AND temperatur*) OR (chang* AND impact*) OR (warm* AND global*) OR (chang* AND water*) OR (climat* AND adapt*) OR (increas* AND temperatur*) OR (climat* AND sea*) OR (climat* AND rainfal*) OR (chang* AND forest*) OR (impact* AND year*) OR (climat* AND precipit*) OR (climatechang* AND ecosystem*) OR (impact* AND variabl*) OR (chang* AND ecolog*) OR (water* AND sea*) OR (climat chang* AND warm*) OR (impact* AND data*) OR (extrem* AND rainfal*) OR (chang* AND ecosystem*) OR (chang* AND communiti*) OR (region* AND water*) OR (increas* AND impact*) OR (increas* AND warm*) OR (rang* AND ecolog*))\n"
     ]
    }
   ],
   "source": [
    "t = \" OR \".join([f\"({x})\" for x in c])\n",
    "q = f\"TS=({t})\"\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Background. A number of factors have recently caused mass coral mortality events in all of the world's tropical oceans. However, little is known about the timing, rate or spatial variability of the loss of reef-building corals, especially in the Indo-Pacific, which contains 75% of the world's coral reefs. Methodology/Principle Findings. We compiled and analyzed a coral cover database of 6001 quantitative surveys of 2667 Indo-Pacific coral reefs performed between 1968 and 2004. Surveys conducted during 2003 indicated that coral cover averaged only 22.1% (95% CI: 20.7, 23.4) and just 7 of 390 reefs surveyed that year had coral cover > 60%. Estimated yearly coral cover loss based on annually pooled survey data was approximately 1% over the last twenty years and 2% between 1997 and 2003 (or 3,168 km(2) per year). The annual loss based on repeated measures regression analysis of a subset of reefs that were monitored for multiple years from 1997 to 2004 was 0.72% (n = 476 reefs, 95% CI: 0.36, 1.08). Conclusions/Significance. The rate and extent of coral loss in the Indo-Pacific are greater than expected. Coral cover was also surprisingly uniform among subregions and declined decades earlier than previously assumed, even on some of the Pacific's most intensely managed reefs. These results have significant implications for policy makers and resource managers as they search for successful models to reverse coral loss. Regional Decline of Coral Cover in the Indo-Pacific: Timing, Extent, and Subregional Comparisons\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[np.where(x.A==0)[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index (353) out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-f434a4b58cab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m353\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/django-tmv/tmv/venv/lib/python3.6/site-packages/scipy/sparse/csr.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0;31m# [i, 1:2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_row_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m             \u001b[0;31m# [i, [1, 2]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0missequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/django-tmv/tmv/venv/lib/python3.6/site-packages/scipy/sparse/csr.py\u001b[0m in \u001b[0;36m_get_row_slice\u001b[0;34m(self, i, cslice)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'index (%d) out of range'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcslice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index (353) out of range"
     ]
    }
   ],
   "source": [
    "np.where(x.A==0)\n",
    "\n",
    "for j in list(find(X[353]))[1]:\n",
    "    print(vocab[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective conservation requires rigorous baselines of pristine conditions to assess the impacts of human activities and to evaluate the efficacy of management. Most coral reefs are moderately to severely degraded by local human activities such as fishing and pollution as well as global change, hence it is difficult to separate local from global effects. To this end, we surveyed coral reefs on uninhabited atolls in the northern Line Islands to provide a baseline of reef community structure, and on increasingly populated atolls to document changes associated with human activities. We found that top predators and reef-building organisms dominated unpopulated Kingman and Palmyra, while small planktivorous fishes and fleshy algae dominated the populated atolls of Tabuaeran and Kiritimati. Sharks and other top predators overwhelmed the fish assemblages on Kingman and Palmyra so that the biomass pyramid was inverted (top-heavy). In contrast, the biomass pyramid at Tabuaeran and Kiritimati exhibited the typical bottom-heavy pattern. Reefs without people exhibited less coral disease and greater coral recruitment relative to more inhabited reefs. Thus, protection from overfishing and pollution appears to increase the resilience of reef ecosystems to the effects of global warming. Baselines and Degradation of Coral Reefs in the Northern Line Islands\n"
     ]
    }
   ],
   "source": [
    "# Climate words\n",
    "\n",
    "cwords = [\"climate change\",\"climat\"]\n",
    "for t in texts:\n",
    "    tc = False\n",
    "    for w in cwords:\n",
    "        if w in t.lower():\n",
    "            tc=True\n",
    "    if not tc:\n",
    "        print(t)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,c in enumerate(combinations(indices, 4)):\n",
    "#     x = sum([X[:,x] for x in c])\n",
    "#     l = len(x.data)\n",
    "#     if l > 740:\n",
    "#         print(f'{\" OR \".join([vocab[x] for x in c])} returns {l}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209\n",
      "218\n",
      "225\n",
      "228\n",
      "229\n",
      "6\n",
      "chang* OR region* OR area* OR impact* OR sea* OR climat* returns 229\n"
     ]
    }
   ],
   "source": [
    "ids = []\n",
    "for i in range(10):\n",
    "    if i==0:\n",
    "        x = X[:,indices[0]]\n",
    "        ids.append(indices[0])\n",
    "        continue\n",
    "    max_x = len(x.data)\n",
    "    max_ind = None\n",
    "    for ind in indices:\n",
    "        if ind in ids:\n",
    "            continue\n",
    "        tmp_x = x + X[:,ind]\n",
    "        if len(tmp_x.data) > max_x:\n",
    "            max_x = len(tmp_x.data)\n",
    "            max_ind = ind\n",
    "    if not max_ind:\n",
    "        break\n",
    "    else:\n",
    "        x = x + X[:,max_ind]\n",
    "        ids.append(max_ind)\n",
    "        print(max_x)\n",
    "        \n",
    "print(i)\n",
    "print(f'{\" OR \".join([vocab[x]+\"*\" for x in ids])} returns {len(x.data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181\n",
      "204\n",
      "217\n",
      "223\n",
      "226\n",
      "228\n",
      "229\n",
      "8\n",
      "\"increas*\" OR \"use*\" OR \"climat chang*\" OR \"year*\" OR \"show*\" OR \"studi*\" OR \"may*\" OR \"time*\" returns 229\n"
     ]
    }
   ],
   "source": [
    "n_ids = []\n",
    "for i in range(10):\n",
    "    if i==0:\n",
    "        for ind in indices:\n",
    "            if ind not in ids:\n",
    "                x = X[:,ind]\n",
    "                n_ids.append(ind)\n",
    "                break\n",
    "        continue\n",
    "    max_x = len(x.data)\n",
    "    max_ind = None\n",
    "    for ind in indices:\n",
    "        if ind in ids or ind in n_ids:\n",
    "            continue\n",
    "        tmp_x = x + X[:,ind]\n",
    "        if len(tmp_x.data) > max_x:\n",
    "            max_x = len(tmp_x.data)\n",
    "            max_ind = ind\n",
    "    if not max_ind:\n",
    "        break\n",
    "    else:\n",
    "        x = x + X[:,max_ind]\n",
    "        n_ids.append(max_ind)\n",
    "        print(max_x)\n",
    "        \n",
    "print(i)\n",
    "strings = [f'\"{vocab[x]}*\"' for x in n_ids]\n",
    "print(f'{\" OR \".join(strings)} returns {len(x.data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157\n",
      "186\n",
      "203\n",
      "213\n",
      "220\n",
      "224\n",
      "227\n",
      "229\n",
      "9\n",
      "\"temperatur*\" OR \"relat*\" OR \"model*\" OR \"speci*\" OR \"result*\" OR \"period*\" OR \"effect*\" OR \"reserv*\" OR \"condit*\" returns 229\n"
     ]
    }
   ],
   "source": [
    "ids += n_ids\n",
    "n_ids = []\n",
    "for i in range(10):\n",
    "    if i==0:\n",
    "        for ind in indices:\n",
    "            if ind not in ids:\n",
    "                x = X[:,ind]\n",
    "                n_ids.append(ind)\n",
    "                break\n",
    "        continue\n",
    "    max_x = len(x.data)\n",
    "    max_ind = None\n",
    "    for ind in indices:\n",
    "        if ind in ids or ind in n_ids:\n",
    "            continue\n",
    "        tmp_x = x + X[:,ind]\n",
    "        if len(tmp_x.data) > max_x:\n",
    "            max_x = len(tmp_x.data)\n",
    "            max_ind = ind\n",
    "    if not max_ind:\n",
    "        break\n",
    "    else:\n",
    "        x = x + X[:,max_ind]\n",
    "        n_ids.append(max_ind)\n",
    "        print(max_x)\n",
    "        \n",
    "print(i)\n",
    "strings = [f'\"{vocab[x]}*\"' for x in n_ids]\n",
    "print(f'{\" OR \".join(strings)} returns {len(x.data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "179\n",
      "199\n",
      "210\n",
      "217\n",
      "222\n",
      "226\n",
      "228\n",
      "229\n",
      "9\n",
      "warm OR trend OR data OR high OR present OR water OR event OR dynam OR also OR global returns 229\n"
     ]
    }
   ],
   "source": [
    "ids += n_ids\n",
    "n_ids = []\n",
    "for i in range(10):\n",
    "    if i==0:\n",
    "        for ind in indices:\n",
    "            if ind not in ids:\n",
    "                x = X[:,ind]\n",
    "                n_ids.append(ind)\n",
    "                break\n",
    "        continue\n",
    "    max_x = len(x.data)\n",
    "    max_ind = None\n",
    "    for ind in indices:\n",
    "        if ind in ids or ind in n_ids:\n",
    "            continue\n",
    "        tmp_x = x + X[:,ind]\n",
    "        if len(tmp_x.data) > max_x:\n",
    "            max_x = len(tmp_x.data)\n",
    "            max_ind = ind\n",
    "    if not max_ind:\n",
    "        break\n",
    "    else:\n",
    "        x = x + X[:,max_ind]\n",
    "        n_ids.append(max_ind)\n",
    "        print(max_x)\n",
    "        \n",
    "print(i)\n",
    "print(f'{\" OR \".join([vocab[x] for x in n_ids])} returns {len(x.data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n",
      "169\n",
      "187\n",
      "201\n",
      "212\n",
      "221\n",
      "225\n",
      "228\n",
      "229\n",
      "9\n",
      "respons OR signific OR observ OR rate OR variabl OR recent OR provid OR differ OR sever OR shift returns 229\n"
     ]
    }
   ],
   "source": [
    "ids += n_ids\n",
    "n_ids = []\n",
    "for i in range(10):\n",
    "    if i==0:\n",
    "        for ind in indices:\n",
    "            if ind not in ids:\n",
    "                x = X[:,ind]\n",
    "                n_ids.append(ind)\n",
    "                break\n",
    "        continue\n",
    "    max_x = len(x.data)\n",
    "    max_ind = None\n",
    "    for ind in indices:\n",
    "        if ind in ids or ind in n_ids:\n",
    "            continue\n",
    "        tmp_x = x + X[:,ind]\n",
    "        if len(tmp_x.data) > max_x:\n",
    "            max_x = len(tmp_x.data)\n",
    "            max_ind = ind\n",
    "    if not max_ind:\n",
    "        break\n",
    "    else:\n",
    "        x = x + X[:,max_ind]\n",
    "        n_ids.append(max_ind)\n",
    "        print(max_x)\n",
    "        \n",
    "print(i)\n",
    "print(f'{\" OR \".join([vocab[x] for x in n_ids])} returns {len(x.data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmv",
   "language": "python",
   "name": "tmv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
